# Django DuckDB Benchmark

Um projeto de benchmark para comparar performance entre Django ORM tradicional e DuckDB em operaÃ§Ãµes analÃ­ticas e consultas complexas.

## ğŸ“‹ Sobre o Projeto

Este benchmark foi desenvolvido para avaliar e comparar o desempenho entre:
- **Django ORM** com PostgreSQL/SQLite
- **DuckDB** como banco de dados analÃ­tico
- OperaÃ§Ãµes hÃ­bridas Django + DuckDB

## ğŸ¯ Objetivos

- Comparar tempos de execuÃ§Ã£o em consultas analÃ­ticas
- Avaliar performance em agregaÃ§Ãµes complexas
- Medir eficiÃªncia no processamento de grandes datasets
- Identificar casos de uso ideais para cada abordagem

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.8+**
- **Django 4.2+**
- **DuckDB** - Banco de dados analÃ­tico em memÃ³ria
- **PostgreSQL** - Para comparaÃ§Ã£o com Django ORM
- **Pandas** - Para manipulaÃ§Ã£o de dados
- **Matplotlib/Plotly** - Para visualizaÃ§Ã£o dos resultados

## ğŸ“Š Tipos de Benchmark

### 1. OperaÃ§Ãµes de AgregaÃ§Ã£o
- COUNT, SUM, AVG, MAX, MIN
- GROUP BY com mÃºltiplas dimensÃµes
- Consultas com HAVING

### 2. Consultas AnalÃ­ticas
- Window functions
- CTEs (Common Table Expressions)
- Subconsultas complexas

### 3. OperaÃ§Ãµes de JOIN
- INNER JOIN
- LEFT/RIGHT JOIN
- MÃºltiplos JOINs

### 4. Processamento de Grandes Volumes
- Datasets de 100K+ registros
- OperaÃ§Ãµes de ETL
- AnÃ¡lises temporais

## ğŸš€ Como Executar

### PrÃ©-requisitos

```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar banco de dados
python manage.py migrate

# Gerar dados de teste
python manage.py generate_test_data
```

### Executando os Benchmarks

```bash
# Executar todos os benchmarks
python manage.py run_benchmark

# Executar benchmark especÃ­fico
python manage.py run_benchmark --test=aggregation

# Gerar relatÃ³rio
python manage.py generate_report
```

## ğŸ“ˆ Estrutura dos Dados

```python
# Exemplo de modelo para benchmark
class SalesRecord(models.Model):
    date = models.DateField()
    product_id = models.IntegerField()
    category = models.CharField(max_length=50)
    quantity = models.IntegerField()
    price = models.DecimalField(max_digits=10, decimal_places=2)
    region = models.CharField(max_length=50)
```

## ğŸ“‹ Comandos DisponÃ­veis

```bash
# Gerar dados de teste
python manage.py generate_test_data --size=100000

# Executar benchmark de agregaÃ§Ã£o
python manage.py benchmark_aggregation

# Executar benchmark de joins
python manage.py benchmark_joins

# Limpar dados de teste
python manage.py cleanup_test_data

# Exportar resultados
python manage.py export_results --format=json
```

## ğŸ“Š Resultados Esperados

O benchmark irÃ¡ gerar mÃ©tricas como:
- **Tempo de execuÃ§Ã£o** (ms)
- **Uso de memÃ³ria** (MB)
- **Throughput** (registros/segundo)
- **Escalabilidade** por tamanho do dataset

## ğŸ”§ ConfiguraÃ§Ã£o

### settings.py
```python
# ConfiguraÃ§Ãµes especÃ­ficas do benchmark
BENCHMARK_SETTINGS = {
    'DUCKDB_PATH': ':memory:',  # ou caminho para arquivo
    'TEST_DATA_SIZE': 100000,
    'REPEAT_COUNT': 5,
    'EXPORT_FORMAT': 'json'
}
```

### requirements.txt
```txt
Django>=4.2.0
duckdb>=0.8.0
pandas>=1.5.0
psycopg2-binary>=2.9.0
matplotlib>=3.6.0
plotly>=5.11.0
```

## ğŸ“ Estrutura do Projeto

```
django-duckdb-benchmark/
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ management/
â”‚   â”‚   â””â”€â”€ commands/
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ duckdb_queries.py
â”‚   â””â”€â”€ django_queries.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test_datasets/
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ reports/
â”‚   â””â”€â”€ charts/
â”œâ”€â”€ static/
â”œâ”€â”€ templates/
â”œâ”€â”€ manage.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ¯ Casos de Uso Testados

### 1. E-commerce Analytics
- AnÃ¡lise de vendas por perÃ­odo
- Top produtos por categoria
- AnÃ¡lise de comportamento do cliente

### 2. Financial Data Processing
- CÃ¡lculos de mÃ©tricas financeiras
- AnÃ¡lise de sÃ©ries temporais
- AgregaÃ§Ãµes por perÃ­odo

### 3. Log Analysis
- Processamento de logs de aplicaÃ§Ã£o
- AnÃ¡lise de performance
- DetecÃ§Ã£o de padrÃµes

## ğŸ“ˆ Monitoramento de Performance

O projeto inclui:
- Profiling detalhado de consultas
- MediÃ§Ã£o de uso de CPU e memÃ³ria
- AnÃ¡lise de planos de execuÃ§Ã£o
- ComparaÃ§Ã£o de Ã­ndices

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“ To-Do

- [ ] Implementar benchmark com Apache Arrow
- [ ] Adicionar suporte a ClickHouse
- [ ] Criar dashboard web interativo
- [ ] Implementar testes automatizados
- [ ] Adicionar mais tipos de consultas analÃ­ticas

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ‘¥ Autor

**Ernane** - [@ernane2022](https://github.com/ernane2022)

## ğŸ”— Links Ãšteis

- [DuckDB Documentation](https://duckdb.org/docs/)
- [Django ORM Documentation](https://docs.djangoproject.com/en/stable/topics/db/)
- [DB Benchmark Project](https://duckdblabs.github.io/db-benchmark/)

---

â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela no repositÃ³rio!